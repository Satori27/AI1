import os
import json
import requests
from dotenv import load_dotenv
load_dotenv()

def get_wikipedia_content(qid, lang="ru"):
    wikidata_url = f"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json"
    wd_response = requests.get(wikidata_url).json()
    
    sitelinks = wd_response["entities"][qid].get("sitelinks", {})
    wiki_key = f"{lang}wiki"
    if wiki_key not in sitelinks:
        return f"–°—Ç–∞—Ç—å—è –Ω–∞ {lang} –í–∏–∫–∏–ø–µ–¥–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è {qid}."
    
    title = sitelinks[wiki_key]["title"]
    
    wikipedia_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{title}"
    wp_response = requests.get(wikipedia_url).json()
    
    if "extract" not in wp_response:
        return f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ '{title}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    return f"üìÑ {title}\n\n{wp_response['extract']}"

def get_wikidata_id(article_name, lang="ru"):
    url = "https://query.wikidata.org/sparql"
    query = f"""
    SELECT ?item WHERE {{
      ?item rdfs:label "{article_name}"@{lang}.
    }}
    """
    headers = {
        "User-Agent": "PythonWikidataClient/1.0",
        "Accept": "application/json"
    }
    response = requests.get(url, params={"query": query, "format": "json"}, headers=headers)
    
    if response.status_code != 200:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")

    data = response.json()
    results = data.get("results", {}).get("bindings", [])
    
    if not results:
        return f"–°—Ç–∞—Ç—å—è '{article_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Wikidata."

    item_id = results[0]["item"]["value"].split("/")[-1]
    
    text = get_wikipedia_content(item_id, lang=lang)
    
    return f"üîπ ID —Å—Ç–∞—Ç—å–∏ '{article_name}' –≤ Wikidata: {item_id}\n\n{text}"

def build_yandexgpt_prompt(folder_id, user_text):
    data = {
        "modelUri": f"gpt://{folder_id}/yandexgpt/latest",
        "completionOptions": {
            "stream": False,
            "temperature": 0.6,
            "maxTokens": "5000",
            "reasoningOptions": {
                "mode": "DISABLED"
            }
        },
        "messages": [
            {
                "role": "system",
                "text": "–í—ã–¥–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ MarkDown –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–Ω–µ–º—É —Ä–µ–±–µ–Ω–∫—É, –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ markdown –ø–æ –º–∞–∫—Å–∏–º—É–º—É"
            },
            {
                "role": "user",
                "text": user_text
            }
        ]
    }
    return json.dumps(data, ensure_ascii=False)

def call_yandex_gpt(prompt_json):
    """
    –î–µ–ª–∞–µ—Ç POST-–∑–∞–ø—Ä–æ—Å (—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –∫–æ–º–∞–Ω–¥—ã curl):
      curl \
        --request POST \
        --header "Content-Type: application/json" \
        --header "Authorization: Bearer ${IAM_TOKEN}" \
        --data "@prompt.json" \
        "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"

    prompt_json ‚Äì –≥–æ—Ç–æ–≤–∞—è JSON-—Å—Ç—Ä–æ–∫–∞, –∫–æ—Ç–æ—Ä—É—é –æ—Ç–ø—Ä–∞–≤–∏–º –≤ –∑–∞–ø—Ä–æ—Å.
    """
    iam_token = os.environ.get("IAM_TOKEN")
    if not iam_token:
        raise ValueError("–ù–µ –∑–∞–¥–∞–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è IAM_TOKEN!")

    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {iam_token}"
    }
    
    response = requests.post(url, headers=headers, data=prompt_json)
    response.raise_for_status()
    return response.json()

def main():
    folder_id = os.environ.get("FOLDER_ID")
    if not folder_id:
        print("–ù–µ –∑–∞–¥–∞–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è FOLDER_ID!")
        return
    
    with open("list_diseases.txt", "r", encoding="utf-8") as f:
        diseases = [line.strip() for line in f if line.strip()]
    
    pages_dir = "pages"
    os.makedirs(pages_dir, exist_ok=True)
    
    for disease in diseases:
        print(f"\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞: {disease} ===")
        
        wiki_text = get_wikidata_id(disease, lang="ru")
        print("–ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:")
        print(wiki_text)
        
        prompt_json = build_yandexgpt_prompt(folder_id, wiki_text)
        try:
            result = call_yandex_gpt(prompt_json)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Yandex GPT: {e}")
            continue

        try:
            gpt_text = result["result"]["alternatives"][0]["message"]["text"]
        except (KeyError, IndexError):
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ JSON:", result)
            gpt_text = "–û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –æ—Ç–≤–µ—Ç–∞"

        safe_filename = disease.replace(" ", "_")
        md_filename = os.path.join(pages_dir, f"{safe_filename}.md")
        
        with open(md_filename, "w", encoding="utf-8") as md_file:
            md_file.write(gpt_text)
        
        print(f"–û—Ç–≤–µ—Ç –∑–∞–ø–∏—Å–∞–Ω –≤ —Ñ–∞–π–ª: {md_filename}")


if __name__ == "__main__":
    main()
